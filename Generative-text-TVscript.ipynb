{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FRIENDS TV SCRIPT GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "def load_data():\n",
    "    data_dir = './data/friends.txt'\n",
    "    input_file = os.path.join(data_dir)\n",
    "    with open(input_file, 'r', encoding='utf8') as file:\n",
    "        text= file.read()\n",
    "    return text\n",
    "text = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 625513\n",
      "Number of lines: 100218\n",
      "Average number of words in each line: 6.24152347881618\n",
      "Sample text from index 0 to index 500\n",
      "[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\n",
      "\n",
      "Monica: There's nothing to tell! He's just some guy I work with!\n",
      "\n",
      "Joey: C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "\n",
      "Chandler: All right Joey, be nice.  So does he have a hump? A hump and a hairpiece?\n",
      "\n",
      "Phoebe: Wait, does he eat chalk?\n",
      "\n",
      "(They all stare, bemused.)\n",
      "\n",
      "Phoebe: Just, 'cause, I don't want her to go through what I went through with Carl- oh!\n",
      "\n",
      "Monica: Okay, everybody relax. This is n\n"
     ]
    }
   ],
   "source": [
    "### Dataset statistics\n",
    "w = []\n",
    "for word in text.split():\n",
    "    w.append(None)\n",
    "print(\"Number of unique words: {}\".format(len(w)))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "\n",
    "word_count_per_line = []\n",
    "for line in lines:\n",
    "    word_count_per_line.append(len(line.split()))\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_per_line)))\n",
    "\n",
    "print(\"Sample text from index {} to index {}\".format(0,500))\n",
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. create lookup table - which will include creating two dictionaries. One to map the words to integers and other to map integers to words\n",
    "2. tokenize punctuation - The words are split by a space delimiter, when punctuations are a part of the word it will create multiple ids for the same word. The token_lookup will create a dictionary which has symbols as keys, the tokens as values. This dictionary can be used to tokenize the symbols/punctuations thus separating the words from tokens and making it easier for the neural network to predict next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_table(text):\n",
    "    word_count = Counter(text)\n",
    "    sorted_word_count = sorted(word_count, key= word_count.get, reverse=True)\n",
    "    \n",
    "    int_to_vocab = {num: word for num, word in enumerate(sorted_word_count)}\n",
    "    vocab_to_int = {word: num for num, word in int_to_vocab.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    tokens = dict()\n",
    "    tokens['.'] = '||PERIOD||'\n",
    "    tokens[','] = '||COMMA||'\n",
    "    tokens['\"'] = '||QUOTATION_MARK||'\n",
    "    tokens[';'] = '||SEMICOLON||'\n",
    "    tokens['!'] = '||EXCLAMATION_MARK||'\n",
    "    tokens['?'] = '||QUESTION_MARK||'\n",
    "    tokens['('] = '||LEFT_PAREN||'\n",
    "    tokens[')'] = '||RIGHT_PAREN||'\n",
    "    tokens['?'] = '||QUESTION_MARK||'\n",
    "    tokens['-'] = '||DASH||'\n",
    "    tokens['\\n'] = '||NEW_LINE||'\n",
    "    return tokens   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data and save it\n",
    "def preprocess_data():\n",
    "    SPECIAL_WORDS = {'PADDING': '||PAD||'}\n",
    "    \n",
    "    text = load_data()\n",
    "    token_dictionary = token_lookup()\n",
    "    \n",
    "    for key,token in token_dictionary.items():\n",
    "        text = text.replace(key, ' {}'.format(token))\n",
    "        \n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    vocab_to_int, int_to_vocab = create_lookup_table(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dictionary), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, GPU available\n"
     ]
    }
   ],
   "source": [
    "gpu_available = torch.cuda.is_available()\n",
    "if(not gpu_available):\n",
    "    print(\"Error, no GPU available\")\n",
    "else:\n",
    "    print(\"Success, GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input - separate the input into chunks of batches and create a data from the features and target tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(words, sequence_length, batch_size):\n",
    "    nwords = len(words)//batch_size\n",
    "    words = words[:len(words)]\n",
    "    y_length = len(words)-sequence_length\n",
    "    x = []\n",
    "    y = []\n",
    "    for index in range(0,y_length):\n",
    "        index_end = index + sequence_length\n",
    "        #features would be from the current index until the end of sequence\n",
    "        x_batch = words[index:index_end]\n",
    "        x.append(x_batch)\n",
    "        #target/predicted would be the next word in the sequence- index_end in this case\n",
    "        y_batch = words[index_end]\n",
    "        y.append(y_batch)\n",
    "        \n",
    "    #create Tensor datasets from both the x and y lists\n",
    "    data = torch.utils.data.TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    return data_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function forward_back_prop will take care of the forward and backward propogation on the neural network. The paramtes that the function takes in are : neural network module, optimizer, Loss function, Target output for the current batch of input.\n",
    "This function returns the latest hidden state tensor and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    # move model to GPU, if available\n",
    "    if(gpu_available):\n",
    "        rnn.cuda()\n",
    "    h = tuple([each.data for each in hidden])\n",
    "\n",
    "    # zero accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if(gpu_available):\n",
    "        inputs, target = inp.cuda(), target.cuda()    \n",
    "    # getting predicted outputs\n",
    "    output, h = rnn(inputs, h)   \n",
    "    #calculate loss\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    # 'clip_grad_norm' helps prevent the exploding gradient problem in RNNs / LSTMs\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function train_rnn will train the Neural network over all the batches for the number of epochs passed to it. It returns the trained rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches):\n",
    "    batch_losses = []   \n",
    "    rnn.train()\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):            \n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break           \n",
    "            #forward and back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "            # printing the current loss and status\n",
    "            if (batch_i % show_every_n_batches) == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20919\n"
     ]
    }
   ],
   "source": [
    "sequence_length =  10 # of words in a sequence\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 200\n",
    "hidden_dim = 250\n",
    "n_layers = 2\n",
    "show_every_n_batches = 2000\n",
    "\n",
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model with hyperparameters and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if gpu_available:\n",
    "    rnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epoch(s)...\n",
      "Epoch:    1/20    Loss: 5.326299237966538\n",
      "\n",
      "Epoch:    1/20    Loss: 4.714436972737312\n",
      "\n",
      "Epoch:    1/20    Loss: 4.532453446269035\n",
      "\n",
      "Epoch:    2/20    Loss: 4.316002941682848\n",
      "\n",
      "Epoch:    2/20    Loss: 4.089063893079758\n",
      "\n",
      "Epoch:    2/20    Loss: 4.064766646385193\n",
      "\n",
      "Epoch:    3/20    Loss: 3.9990834399684547\n",
      "\n",
      "Epoch:    3/20    Loss: 3.8613693948984147\n",
      "\n",
      "Epoch:    3/20    Loss: 3.853468917131424\n",
      "\n",
      "Epoch:    5/20    Loss: 3.6865270157999617\n",
      "\n",
      "Epoch:    5/20    Loss: 3.617316173553467\n",
      "\n",
      "Epoch:    5/20    Loss: 3.628477858185768\n",
      "\n",
      "Epoch:    6/20    Loss: 3.598852265087855\n",
      "\n",
      "Epoch:    6/20    Loss: 3.5425473648309707\n",
      "\n",
      "Epoch:    6/20    Loss: 3.5511225508451463\n",
      "\n",
      "Epoch:    7/20    Loss: 3.5229055145572374\n",
      "\n",
      "Epoch:    7/20    Loss: 3.4750159566402434\n",
      "\n",
      "Epoch:    7/20    Loss: 3.485599455356598\n",
      "\n",
      "Epoch:    8/20    Loss: 3.4678699115797276\n",
      "\n",
      "Epoch:    8/20    Loss: 3.4274679460525515\n",
      "\n",
      "Epoch:    8/20    Loss: 3.4416489213705064\n",
      "\n",
      "Epoch:    9/20    Loss: 3.416726037302918\n",
      "\n",
      "Epoch:    9/20    Loss: 3.3796466752290724\n",
      "\n",
      "Epoch:    9/20    Loss: 3.3953253728151322\n",
      "\n",
      "Epoch:   10/20    Loss: 3.376316453037005\n",
      "\n",
      "Epoch:   10/20    Loss: 3.342129667758942\n",
      "\n",
      "Epoch:   10/20    Loss: 3.358922740340233\n",
      "\n",
      "Epoch:   11/20    Loss: 3.338253017748941\n",
      "\n",
      "Epoch:   11/20    Loss: 3.3097164545059203\n",
      "\n",
      "Epoch:   11/20    Loss: 3.32851781976223\n",
      "\n",
      "Epoch:   12/20    Loss: 3.304007800144497\n",
      "\n",
      "Epoch:   12/20    Loss: 3.279093257069588\n",
      "\n",
      "Epoch:   12/20    Loss: 3.303474399805069\n",
      "\n",
      "Epoch:   13/20    Loss: 3.2752400618757127\n",
      "\n",
      "Epoch:   13/20    Loss: 3.2561611456871034\n",
      "\n",
      "Epoch:   13/20    Loss: 3.2797811094522475\n",
      "\n",
      "Epoch:   15/20    Loss: 3.2246825030773363\n",
      "\n",
      "Epoch:   15/20    Loss: 3.2101038924455643\n",
      "\n",
      "Epoch:   15/20    Loss: 3.2336438007354738\n",
      "\n",
      "Epoch:   16/20    Loss: 3.204732710961432\n",
      "\n",
      "Epoch:   16/20    Loss: 3.1889253562688826\n",
      "\n",
      "Epoch:   16/20    Loss: 3.2112446064949034\n",
      "\n",
      "Epoch:   17/20    Loss: 3.1854125557607307\n",
      "\n",
      "Epoch:   17/20    Loss: 3.1753821821212767\n",
      "\n",
      "Epoch:   17/20    Loss: 3.1950034909248353\n",
      "\n",
      "Epoch:   18/20    Loss: 3.1657971395003774\n",
      "\n",
      "Epoch:   18/20    Loss: 3.1580618221759797\n",
      "\n",
      "Epoch:   18/20    Loss: 3.1810566331148147\n",
      "\n",
      "Epoch:   19/20    Loss: 3.148327446259515\n",
      "\n",
      "Epoch:   19/20    Loss: 3.1442518309354783\n",
      "\n",
      "Epoch:   19/20    Loss: 3.164167458176613\n",
      "\n",
      "Epoch:   20/20    Loss: 3.141328989701464\n",
      "\n",
      "Epoch:   20/20    Loss: 3.1306688200235366\n",
      "\n",
      "Epoch:   20/20    Loss: 3.1507721773386\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "trained_rnn = load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text \n",
    "\n",
    "The network will take in the prime word and generates/predicts the next set of words of size predict_len with the prime word as the key. It gets the top 5 probable next words and generates the next word with some randomness among the top 5. After it generates each predicted sequence, the function does post-processing to replace the tokens with the original punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if gpu_available:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(gpu_available):\n",
    "            p = p.cpu() \n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        current_seq = current_seq.cpu()\n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Post-processing\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ross: advice, i don’t know what to say.\n",
      "\n",
      "chandler: well, i was going to do that, but i think we need it, okay? you-you-you can be a little bit!\n",
      "\n",
      "ross: i know! i mean you didn’t know anything?!\n",
      "\n",
      "joey: yeah. but then, i don’t know if you didn’t want to tell me that i could get married.\n",
      "\n",
      "joey: well, i’m-i’m gonna go find you and we get the same suite to be alone and we have sex with you. and then we-we don’t think i was just practicing e aunt you and-and we were gonna get married to the airport.\n",
      "\n",
      "phoebe: yeah, i’m sorry. i know.\n",
      "\n",
      "monica: what?! i mean i can’t believe it! i mean you know what? i don’t know, i’m not gonna go get a little bit. i mean it was a little bit. and if you want to be able to get wet a little bit!\n",
      "\n",
      "rachel: oh no, you can’t tell you that i am.\n",
      "\n",
      "joey: oh, i know i know! i am a terrible person, i’m gonna be alone in my apartment! okay? i mean you don’t know what i’m gonna go to the street?\n",
      "\n",
      "monica: well, i think i’m gonna be with a couple of you.\n",
      "\n",
      "ross: oh, yeah-yeah, i know. i don’t know. i know.\n",
      "\n",
      "ross: yeah?\n",
      "\n",
      "phoebe: i guess i think i should.\n",
      "\n",
      "rachel: yeah, i-i think it’s better than that.\n",
      "\n",
      "rachel: i can’t believe it is a real responsibility.\n",
      "\n",
      "joey: okay.(they hug.)\n",
      "\n",
      "monica:(to chandler) oh my god! i know! you-you-you know?!\n",
      "\n",
      "ross: i think so.\n",
      "\n",
      "monica: oh, that’s great. oh, i don’t know why to get pregnant. okay?\n",
      "\n",
      "rachel: well umm, i’m not letting you to get pregnant.\n",
      "\n",
      "rachel: yeah, i guess.\n",
      "\n",
      "ross: oh, i don’t know.\n",
      "\n",
      "ross: yeah. but y’know, i was just thinking about that.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 \n",
    "prime_word = 'ross' \n",
    "SPECIAL_WORDS = {'PADDING': '||PAD||'}\n",
    "pad_word = SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_6.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 100 scenes individually by varying the prime words and process them to generate one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save script to file \n",
    "def save_file(num, generated_script):\n",
    "    f = open(\"final_script_{}.txt\".format(num), \"w\")\n",
    "    f.write(generated_script)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final script- generate 100 scenes and put together\n",
    "gen_length = 500\n",
    "prime_words = ['ross', 'rachel','monica','phoebe','chandler','joey']\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "pad_word = SPECIAL_WORDS['PADDING']\n",
    "\n",
    "#generate the first scene as prime word\n",
    "prime_word = '[scene'\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "save_file(0,generated_script)\n",
    "\n",
    "\n",
    "j = 0\n",
    "#loop through for dialogues starting with each characters\n",
    "for p in range(100):\n",
    "    if (j>len(prime_words)-1):\n",
    "        j = 0\n",
    "    prime_word = prime_words[j]\n",
    "    generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "    #write to output script\n",
    "    save_file(p+1,generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and generate the episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated the final episode text file by writing the 100 generated files into it. I used the following command in the terminal\n",
    "\n",
    "`cat *.txt > Final/The Fake Script.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_prepender(filename, line):\n",
    "    with open(filename, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(line.rstrip('\\r\\n') + '\\n\\n' + content)\n",
    "        e = \"End\"\n",
    "        f.write('\\n\\n' + e.rstrip('\\r\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_prepender('The Fake Script.txt', \"                                    The One with the Fake Script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(file):\n",
    "    s = open(file).read()\n",
    "    s = s.replace('joey', 'Joey')\n",
    "    s = s.replace('chandler','Chandler')\n",
    "    s = s.replace('monica','Monica')\n",
    "    s = s.replace('rachel', 'Rachel')\n",
    "    s = s.replace('ross','Ross')\n",
    "    s = s.replace('phoebe','Phoebe')\n",
    "    f = open(file, 'w')\n",
    "    f.write(s)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file('The Fake Script.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
